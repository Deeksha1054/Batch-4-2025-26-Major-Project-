{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7352272,"sourceType":"datasetVersion","datasetId":4244554}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport os\n\nclass RetinopathyDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        try:\n            image = plt.imread(self.image_paths[idx])\n            label = self.labels[idx]\n            \n            if image.ndim == 2:  # Grayscale\n                image = np.stack([image] * 3, axis=-1)\n            if image.shape[-1] != 3:  # Ensure 3 channels\n                image = np.stack([image[..., 0]] * 3, axis=-1)\n            \n            if self.transform:\n                image = self.transform(image)\n                \n            return image, label\n        except Exception as e:\n            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n            return torch.zeros((3, 224, 224)), self.labels[idx]\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\nclass RetinopathyModel(nn.Module):\n    def __init__(self, num_classes=5):\n        super(RetinopathyModel, self).__init__()\n        self.model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        num_ftrs = self.model.classifier[6].in_features\n        self.model.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, num_classes)\n        )\n        \n    def forward(self, x):\n        return self.model(x)\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    early_stopping = EarlyStopping(patience=patience)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n        for inputs, labels in train_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            train_bar.set_postfix({'loss': running_loss/len(train_loader)})\n        \n        epoch_train_loss = running_loss / len(train_loader)\n        train_accuracy = 100 * correct / total\n        \n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        all_preds = []\n        all_labels = []\n        \n        val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        with torch.no_grad():\n            for inputs, labels in val_bar:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                all_preds.extend(outputs.softmax(dim=1).cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                \n                val_bar.set_postfix({'loss': val_loss/len(val_loader)})\n        \n        epoch_val_loss = val_loss / len(val_loader)\n        val_accuracy = 100 * correct / total\n        \n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        train_accs.append(train_accuracy)\n        val_accs.append(val_accuracy)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n        \n        scheduler.step(epoch_val_loss)\n        early_stopping(epoch_val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            break\n    \n    plot_metrics(all_labels, all_preds, train_losses, val_losses, train_accs, val_accs)\n    print_model_summary(all_labels, all_preds, train_losses, val_losses, train_accs, val_accs, epoch + 1)\n    \n    return model\n\ndef test_model(model_path, test_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = RetinopathyModel(num_classes=5)\n    model.load_state_dict(torch.load(model_path, weights_only=True))\n    model = model.to(device)\n    model.eval()\n    \n    test_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    criterion = nn.CrossEntropyLoss()\n    \n    test_bar = tqdm(test_loader, desc='Testing')\n    with torch.no_grad():\n        for inputs, labels in test_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            all_preds.extend(outputs.softmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            test_bar.set_postfix({'loss': test_loss/len(test_loader)})\n    \n    test_loss = test_loss / len(test_loader)\n    test_accuracy = 100 * correct / total\n    \n    print(\"\\n=== Test Results ===\")\n    print(f'Test Loss: {test_loss:.4f}')\n    print(f'Test Accuracy: {test_accuracy:.2f}%')\n    \n    pred_labels = np.argmax(all_preds, axis=1)\n    report = classification_report(all_labels, pred_labels, \n                                 target_names=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])\n    \n    cm = confusion_matrix(all_labels, pred_labels)\n    cm_df = pd.DataFrame(cm, \n                        index=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],\n                        columns=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])\n    \n    print(\"\\nTest Classification Report:\")\n    print(report)\n    print(\"\\nTest Confusion Matrix:\")\n    print(cm_df)\n    \n    plt.figure(figsize=(8, 6))\n    for i in range(5):\n        fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds)[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Test ROC Curve')\n    plt.legend()\n    plt.show()\n\ndef plot_metrics(labels, preds, train_losses, val_losses, train_accs, val_accs):\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    n_classes = 5\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(np.array(labels) == i, np.array(preds)[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n    \n    plt.subplot(1, 3, 2)\n    cm = confusion_matrix(labels, np.argmax(preds, axis=1))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Val Loss')\n    plt.plot(train_accs, label='Train Acc')\n    plt.plot(val_accs, label='Val Acc')\n    plt.title('Training Metrics')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef print_model_summary(labels, preds, train_losses, val_losses, train_accs, val_accs, epochs_run):\n    print(\"\\n=== Model Performance Summary ===\")\n    \n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    final_train_acc = train_accs[-1]\n    final_val_acc = val_accs[-1]\n    \n    pred_labels = np.argmax(preds, axis=1)\n    report = classification_report(labels, pred_labels, \n                                 target_names=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])\n    \n    roc_auc_scores = {}\n    for i in range(5):\n        fpr, tpr, _ = roc_curve(np.array(labels) == i, np.array(preds)[:, i])\n        roc_auc_scores[f'Class {i}'] = auc(fpr, tpr)\n    \n    summary_data = {\n        'Metric': ['Epochs Run', 'Final Train Loss', 'Final Val Loss', 'Final Train Accuracy', 'Final Val Accuracy'],\n        'Value': [epochs_run, f'{final_train_loss:.4f}', f'{final_val_loss:.4f}', \n                 f'{final_train_acc:.2f}%', f'{final_val_acc:.2f}%']\n    }\n    summary_df = pd.DataFrame(summary_data)\n    \n    print(\"\\nGeneral Metrics:\")\n    print(summary_df.to_string(index=False))\n    \n    print(\"\\nROC AUC Scores:\")\n    for class_name, score in roc_auc_scores.items():\n        print(f\"{class_name}: {score:.4f}\")\n        \n    print(\"\\nDetailed Classification Report:\")\n    print(report)\n    \n    cm = confusion_matrix(labels, pred_labels)\n    cm_df = pd.DataFrame(cm, \n                        index=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],\n                        columns=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])\n    print(\"\\nConfusion Matrix:\")\n    print(cm_df)\n\ndef load_data_from_folders(root_path):\n    root_path = Path(root_path)\n    splits = ['train', 'val', 'test']\n    class_dirs = ['0', '1', '2', '3', '4']\n    \n    datasets = {}\n    \n    for split in splits:\n        image_paths = []\n        labels = []\n        \n        split_path = root_path / split\n        if not split_path.exists():\n            print(f\"Warning: {split_path} does not exist\")\n            continue\n            \n        print(f\"\\nProcessing {split} split:\")\n        for class_dir in class_dirs:\n            class_path = split_path / class_dir\n            if not class_path.exists():\n                print(f\"Warning: {class_path} does not exist\")\n                continue\n                \n            class_label = int(class_dir)\n            class_images = list(class_path.glob(\"*.[jp][pn][eg]\"))\n            \n            print(f\"Class {class_label}: Found {len(class_images)} images\")\n            \n            image_paths.extend(class_images)\n            labels.extend([class_label] * len(class_images))\n        \n        if not image_paths:\n            raise ValueError(f\"No images found in {split} split\")\n            \n        print(f\"Total images in {split}: {len(image_paths)}\")\n        label_counts = pd.Series(labels).value_counts().sort_index()\n        print(f\"Label distribution in {split}:\")\n        print(label_counts)\n        \n        datasets[split] = (image_paths, labels)\n    \n    return datasets\n\ndef main():\n    root_path = \"/kaggle/input/eyepacs-aptos-messidor-diabetic-retinopathy/augmented_resized_V2\"\n    \n    try:\n        datasets = load_data_from_folders(root_path)\n    except ValueError as e:\n        print(f\"Error loading data: {e}\")\n        return\n    \n    train_dataset = RetinopathyDataset(\n        datasets['train'][0], \n        datasets['train'][1], \n        transform=data_transforms['train']\n    )\n    val_dataset = RetinopathyDataset(\n        datasets['val'][0], \n        datasets['val'][1], \n        transform=data_transforms['val']\n    )\n    test_dataset = RetinopathyDataset(\n        datasets['test'][0], \n        datasets['test'][1], \n        transform=data_transforms['test']\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n    \n    labels = datasets['train'][1]\n    class_counts = pd.Series(labels).value_counts().sort_index()\n    total_samples = len(labels)\n    num_classes = 5\n    class_weights = torch.tensor([total_samples / (num_classes * class_counts[i]) for i in range(num_classes)], \n                                dtype=torch.float).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    \n    model = RetinopathyModel(num_classes=5)\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    \n    trained_model = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=50,\n        patience=7\n    )\n    \n    model_path = 'retinopathy_model.pth'\n    torch.save(trained_model.state_dict(), model_path)\n    \n    test_model(model_path, test_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}